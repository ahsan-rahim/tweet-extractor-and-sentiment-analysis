{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPFAk-FVUMz6",
        "colab_type": "text"
      },
      "source": [
        "# **Tweet Sentiment Analyzer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf13gbxFUacI",
        "colab_type": "text"
      },
      "source": [
        "Some necessary libray imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yyDgc7e1rMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from matplotlib import style\n",
        "from sklearn import preprocessing\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IQv2SB8UmyM",
        "colab_type": "text"
      },
      "source": [
        "# Data loading and preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKNlWdrW19j8",
        "colab_type": "code",
        "outputId": "edc4dc0e-d7e3-48bb-c353-9e70855f0c4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "df = pd.read_csv(r'train.csv',encoding='latin-1')\n",
        "tweet, sentiment= df['text'],df['sentiment']\n",
        "\n",
        "\n",
        "\n",
        "df.sentiment.value_counts()\n",
        "\n",
        "\n",
        "df = df[~df.sentiment.str.contains('\\|')]\n",
        "df = df[df.sentiment != 'nocode']\n",
        "df = df[df.sentiment != 'not-relevant']\n",
        "\n",
        "\n",
        "tweet, sentiment= df['text'],df['sentiment']\n",
        "df.sentiment.value_counts()\n",
        "\n",
        "\n",
        "print(len (sentiment),len (tweet))\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1267 1267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MitscUkjU0OU",
        "colab_type": "text"
      },
      "source": [
        "# Vectorizing Data using TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoFHVYzV2sci",
        "colab_type": "code",
        "outputId": "2a47f5b5-7a76-4814-8bc9-5b1a6017675a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stopwords = set(\"from for are & of the a is we they @ in on an . if has had have and or as but was were he she it but and\".split())\n",
        "\n",
        "#vectorizer = CountVectorizer(ngram_range=(1,2), stop_words=stopwords.words('english'))\n",
        "#vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words=stopwords.words('english'))\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words=stopwords)\n",
        "#features = vectorizer.fit_transform(review)\n",
        "vectorizer = vectorizer.fit(tweet.values.astype('U'))\n",
        "features = vectorizer.transform(tweet.values.astype('U'))\n",
        "features_nd = features.toarray()\n",
        "\n",
        "\n",
        "print(len(features_nd[0]))\n",
        "\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVOLTReZVDnK",
        "colab_type": "text"
      },
      "source": [
        "# Label Encoding and Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNZSccjC28Xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = preprocessing.LabelEncoder()\n",
        "encoder = encoder.fit(sentiment)\n",
        "labels = encoder.transform(sentiment)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test  = train_test_split(features_nd, labels, train_size=.70,random_state=134, stratify=df.sentiment.values)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84HVwa0AR6G0",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1cg75DJ3Sid",
        "colab_type": "code",
        "outputId": "b50bc613-133f-4c8a-f6fb-608ea3ce9b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "nb = MultinomialNB(alpha=0.21)\n",
        "nb.fit(x_train, y_train)\n",
        "nb_pred = nb.predict(x_test)\n",
        "accuracy_score(y_test, nb_pred)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.910761154855643"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pgnL1tUSG55",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI4GJHLf3VkT",
        "colab_type": "code",
        "outputId": "650c5bc2-ac5c-4940-acfb-65ffefb2a7ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\n",
        "rf = RandomForestClassifier(verbose=True)\n",
        "rf.fit(x_train, y_train)\n",
        "rf_pred = rf.predict(x_test)\n",
        "accuracy_score(y_test, rf_pred)\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9081364829396326"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arVr3ixGV1Qd",
        "colab_type": "text"
      },
      "source": [
        "# K-Nearest Neighbour"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDMsscYP3YIE",
        "colab_type": "code",
        "outputId": "bbd43293-9d1c-49ce-8ba7-1e6db59591a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "knn = KNeighborsClassifier( n_neighbors=5, weights='distance' )\n",
        "knn.fit(x_train, y_train)\n",
        "knn_pred = knn.predict(x_test)\n",
        "accuracy_score(y_test, knn_pred)\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.910761154855643"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBdLviaoV6Cq",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaCZ_Xnm3bEk",
        "colab_type": "code",
        "outputId": "cef4cef1-dfb7-4fc5-8f80-1e1db3a2ccf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=20, learning_rate=0.095 ,verbose=True)\n",
        "gb.fit(x_train, y_train)\n",
        "gb_pred = gb.predict(x_test)\n",
        "accuracy_score(y_test, gb_pred)\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss   Remaining Time \n",
            "         1         302.4260           46.58s\n",
            "         2         276.6801           42.83s\n",
            "         3         255.7457           40.90s\n",
            "         4         229.8184           38.62s\n",
            "         5         207.3746           36.42s\n",
            "         6         187.6178           34.18s\n",
            "         7         171.8021           31.88s\n",
            "         8         161.0296           29.49s\n",
            "         9         150.8930           27.13s\n",
            "        10         142.0803           24.57s\n",
            "        20         105.9333            0.00s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9028871391076115"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGUqGHJ2V95a",
        "colab_type": "text"
      },
      "source": [
        "# Multi-Layer Perceptron (Neural Network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mf8KxOs3kAU",
        "colab_type": "code",
        "outputId": "aa215d30-c48a-4037-f0a2-f2f5b3de7d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=99, verbose=True)\n",
        "mlp.fit(x_train, y_train)\n",
        "mlp_pred = mlp.predict(x_test)\n",
        "accuracy_score(y_test, mlp_pred)\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.53906989\n",
            "Iteration 2, loss = 1.43703120\n",
            "Iteration 3, loss = 1.32792211\n",
            "Iteration 4, loss = 1.20392135\n",
            "Iteration 5, loss = 1.06970509\n",
            "Iteration 6, loss = 0.93222188\n",
            "Iteration 7, loss = 0.79679651\n",
            "Iteration 8, loss = 0.67009562\n",
            "Iteration 9, loss = 0.55796078\n",
            "Iteration 10, loss = 0.46164664\n",
            "Iteration 11, loss = 0.38386292\n",
            "Iteration 12, loss = 0.32209132\n",
            "Iteration 13, loss = 0.27393038\n",
            "Iteration 14, loss = 0.23585467\n",
            "Iteration 15, loss = 0.20558630\n",
            "Iteration 16, loss = 0.18100567\n",
            "Iteration 17, loss = 0.16031315\n",
            "Iteration 18, loss = 0.14243863\n",
            "Iteration 19, loss = 0.12686048\n",
            "Iteration 20, loss = 0.11330128\n",
            "Iteration 21, loss = 0.10133174\n",
            "Iteration 22, loss = 0.09066126\n",
            "Iteration 23, loss = 0.08137394\n",
            "Iteration 24, loss = 0.07307426\n",
            "Iteration 25, loss = 0.06593868\n",
            "Iteration 26, loss = 0.05943189\n",
            "Iteration 27, loss = 0.05396792\n",
            "Iteration 28, loss = 0.04896791\n",
            "Iteration 29, loss = 0.04463764\n",
            "Iteration 30, loss = 0.04097735\n",
            "Iteration 31, loss = 0.03767039\n",
            "Iteration 32, loss = 0.03466884\n",
            "Iteration 33, loss = 0.03213687\n",
            "Iteration 34, loss = 0.02975583\n",
            "Iteration 35, loss = 0.02763778\n",
            "Iteration 36, loss = 0.02573152\n",
            "Iteration 37, loss = 0.02405834\n",
            "Iteration 38, loss = 0.02256601\n",
            "Iteration 39, loss = 0.02106497\n",
            "Iteration 40, loss = 0.01979577\n",
            "Iteration 41, loss = 0.01866394\n",
            "Iteration 42, loss = 0.01755637\n",
            "Iteration 43, loss = 0.01661121\n",
            "Iteration 44, loss = 0.01567957\n",
            "Iteration 45, loss = 0.01485547\n",
            "Iteration 46, loss = 0.01409928\n",
            "Iteration 47, loss = 0.01339819\n",
            "Iteration 48, loss = 0.01275788\n",
            "Iteration 49, loss = 0.01216009\n",
            "Iteration 50, loss = 0.01159760\n",
            "Iteration 51, loss = 0.01107429\n",
            "Iteration 52, loss = 0.01059505\n",
            "Iteration 53, loss = 0.01014366\n",
            "Iteration 54, loss = 0.00973203\n",
            "Iteration 55, loss = 0.00934091\n",
            "Iteration 56, loss = 0.00898177\n",
            "Iteration 57, loss = 0.00864521\n",
            "Iteration 58, loss = 0.00833116\n",
            "Iteration 59, loss = 0.00802901\n",
            "Iteration 60, loss = 0.00775215\n",
            "Iteration 61, loss = 0.00749785\n",
            "Iteration 62, loss = 0.00724647\n",
            "Iteration 63, loss = 0.00701042\n",
            "Iteration 64, loss = 0.00678973\n",
            "Iteration 65, loss = 0.00658332\n",
            "Iteration 66, loss = 0.00637490\n",
            "Iteration 67, loss = 0.00618530\n",
            "Iteration 68, loss = 0.00600867\n",
            "Iteration 69, loss = 0.00584099\n",
            "Iteration 70, loss = 0.00567627\n",
            "Iteration 71, loss = 0.00552411\n",
            "Iteration 72, loss = 0.00537750\n",
            "Iteration 73, loss = 0.00523704\n",
            "Iteration 74, loss = 0.00510613\n",
            "Iteration 75, loss = 0.00498217\n",
            "Iteration 76, loss = 0.00485891\n",
            "Iteration 77, loss = 0.00474400\n",
            "Iteration 78, loss = 0.00463418\n",
            "Iteration 79, loss = 0.00452978\n",
            "Iteration 80, loss = 0.00442652\n",
            "Iteration 81, loss = 0.00432894\n",
            "Iteration 82, loss = 0.00423601\n",
            "Iteration 83, loss = 0.00414188\n",
            "Iteration 84, loss = 0.00405460\n",
            "Iteration 85, loss = 0.00397121\n",
            "Iteration 86, loss = 0.00389094\n",
            "Iteration 87, loss = 0.00381305\n",
            "Iteration 88, loss = 0.00373790\n",
            "Iteration 89, loss = 0.00366788\n",
            "Iteration 90, loss = 0.00359836\n",
            "Iteration 91, loss = 0.00353091\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.916010498687664"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmqXLUtwWGdg",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfmfE9nc5LB_",
        "colab_type": "code",
        "outputId": "f8395233-5c62-461b-fedc-4f815134e86d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lr = svm.SVC(C=1,degree=1, kernel = 'poly', verbose=True)\n",
        "lr.fit(x_train, y_train)\n",
        "lr_pred = lr.predict(x_test)\n",
        "accuracy_score(y_test, lr_pred)\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.905511811023622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rZwSq9iSZv2",
        "colab_type": "text"
      },
      "source": [
        "# Tweet Extractor\n",
        "\n",
        "The following code is used to extract tweets using the Twitter API\n",
        "\n",
        "\n",
        "Note: Please enter valid twitter API keys in the twitter_credentials.py file before running this app. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfxWklrPRlCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tweepy import API \n",
        "from tweepy import Cursor\n",
        "from tweepy.streaming import StreamListener\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy import Stream\n",
        " \n",
        "import twitter_credentials\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "################################# ENTER VALID KEYS IN twitter_credentials.py BEFORE RUNNING THIS APP ###########################\n",
        "\n",
        "\n",
        "class TwitterClient():\n",
        "    def __init__(self, twitter_user=None):\n",
        "        self.auth = TwitterAuthenticator().authenticate_twitter_app()\n",
        "        self.twitter_client = API(self.auth)\n",
        "\n",
        "        self.twitter_user = twitter_user\n",
        "\n",
        "    def get_twitter_client_api(self):\n",
        "        return self.twitter_client\n",
        "\n",
        "    def get_user_timeline_tweets(self, num_tweets):\n",
        "        tweets = []\n",
        "        for tweet in Cursor(self.twitter_client.user_timeline, id=self.twitter_user).items(num_tweets):\n",
        "            tweets.append(tweet)\n",
        "        return tweets\n",
        "\n",
        "    def get_friend_list(self, num_friends):\n",
        "        friend_list = []\n",
        "        for friend in Cursor(self.twitter_client.friends, id=self.twitter_user).items(num_friends):\n",
        "            friend_list.append(friend)\n",
        "        return friend_list\n",
        "\n",
        "    def get_home_timeline_tweets(self, num_tweets):\n",
        "        home_timeline_tweets = []\n",
        "        for tweet in Cursor(self.twitter_client.home_timeline, id=self.twitter_user).items(num_tweets):\n",
        "            home_timeline_tweets.append(tweet)\n",
        "        return home_timeline_tweets\n",
        "\n",
        "\n",
        "class TwitterAuthenticator():\n",
        "\n",
        "    def authenticate_twitter_app(self):\n",
        "        auth = OAuthHandler(twitter_credentials.CONSUMER_KEY, twitter_credentials.CONSUMER_SECRET)\n",
        "        auth.set_access_token(twitter_credentials.ACCESS_TOKEN, twitter_credentials.ACCESS_TOKEN_SECRET)\n",
        "        return auth\n",
        "\n",
        "\n",
        "class TwitterStreamer():\n",
        "    \"\"\"\n",
        "    Class for streaming and processing live tweets.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.twitter_autenticator = TwitterAuthenticator()    \n",
        "\n",
        "    def stream_tweets(self, fetched_tweets_filename, hash_tag_list):\n",
        "        # This handles Twitter authetification and the connection to Twitter Streaming API\n",
        "        listener = TwitterListener(fetched_tweets_filename)\n",
        "        auth = self.twitter_autenticator.authenticate_twitter_app() \n",
        "        stream = Stream(auth, listener)\n",
        "\n",
        "        # This line filter Twitter Streams to capture data by the keywords: \n",
        "        stream.filter(track=hash_tag_list)\n",
        "\n",
        "\n",
        "class TwitterListener(StreamListener):\n",
        "    \"\"\"\n",
        "    This is a basic listener that just prints received tweets to stdout.\n",
        "    \"\"\"\n",
        "    def __init__(self, fetched_tweets_filename):\n",
        "        self.fetched_tweets_filename = fetched_tweets_filename\n",
        "\n",
        "    def on_data(self, data):\n",
        "        try:\n",
        "            print(data)\n",
        "            with open(self.fetched_tweets_filename, 'a') as tf:\n",
        "                tf.write(data)\n",
        "            return True\n",
        "        except BaseException as e:\n",
        "            print(\"Error on_data %s\" % str(e))\n",
        "        return True\n",
        "          \n",
        "    def on_error(self, status):\n",
        "        if status == 420:\n",
        "            # Returning False on_data method in case rate limit occurs.\n",
        "            return False\n",
        "        print(status)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Twitter_Extractor( keyword , Number_of_tweets):\n",
        "    twitter_client = TwitterClient()\n",
        "    tweet_analyzer = TweetAnalyzer()\n",
        "\n",
        "    api = twitter_client.get_twitter_client_api()\n",
        "\n",
        "\n",
        "    tweets = api.search(q=keyword, lang='en' , count = Number_of_tweets)\n",
        "    count=0;\n",
        "    return df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['text'])\n",
        "\n",
        "\n",
        "\n",
        "#Driver Code which fetches 50 tweets with the keyword Trump in it.\n",
        "Tweets= Twitter_Extractor('Trump' , 50)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeWZABSd_LNg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "9df10d5a-ef1b-4708-c781-45bb2c515326"
      },
      "source": [
        "#Tweets= tweet[1:50]\n",
        "Tweets=Tweets['text']\n",
        "\n",
        "features = vectorizer.transform(Tweets.values.astype('U'))\n",
        "features_nd = features.toarray()\n",
        "#print(Tweets)\n",
        "print(len(features_nd[0]))\n",
        "\n",
        "pred = mlp.predict(features_nd)\n",
        "\n",
        "senti= encoder.inverse_transform(pred)\n",
        "\n",
        "data= {'Tweet': Tweets  , 'Predicted Sentiment': senti}\n",
        "\n",
        "result=  pd.DataFrame(data)\n",
        "print(result)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15983\n",
            "                                                 Tweet Predicted Sentiment\n",
            "2    @SelectShowcase @Tate_StIves ... Replace with ...               happy\n",
            "3    @Sofabsports thank you for following me back. ...               happy\n",
            "4    @britishmuseum @TudorHistory What a beautiful ...               happy\n",
            "5    @NationalGallery @ThePoldarkian I have always ...               happy\n",
            "9    Lucky @FitzMuseum_UK! Good luck @MirandaStearn...               happy\n",
            "12   Yr 9 art students are off to the @britishmuseu...               happy\n",
            "18                @BarbyWT @britishmuseum so beautiful               happy\n",
            "20                       @britishmuseum awesome museum               happy\n",
            "21   @nationalgallery #AskTheGallery Why do you pay...               angry\n",
            "26                        @britishmuseum soo beautiful               happy\n",
            "27               @NationalGallery I do, I do, I do. :)               happy\n",
            "28   @tateliverpool one of my favourite paintings! ...               happy\n",
            "30   The Magna Carta exhibition @britishmuseum was ...               happy\n",
            "33   Kudos, @FitzMuseum_UK, for a top-flight WC exh...               happy\n",
            "36   @stiveshouse @MillenniumArt @Tate_StIves @Trem...               happy\n",
            "40   Enjoying the amazing collections @britishmuseu...               happy\n",
            "42    @NationalGallery Beautiful painting.  Thank you.               happy\n",
            "43   #definingbeauty @britishmuseum a truly breatht...               happy\n",
            "49   @laiyaharvey thank you for the favourite, foll...               happy\n",
            "50   Treats for the end of June @HeatonsArtTrail fo...               happy\n",
            "52   @tatteredstones @kettlesyard One of the best p...               happy\n",
            "55           @dr_shibley @britishmuseum Awwh - possum!               happy\n",
            "57   Wonderful experience, hearing Tim Knoxâs #ob...               happy\n",
            "59   Hooray! @MooseAllain wins Evolver Prize 2015! ...               happy\n",
            "60   Fabulous Enid Blyton exhibition @PlymouthMuseu...               happy\n",
            "66   @7stories Enid Blyton exhib'n opens @PlymouthM...               happy\n",
            "67   KETTLE'S YARD: ANTIMUSEUM - meet the Archivist...               happy\n",
            "69   A week writing reports about reports suddenly ...               happy\n",
            "71   Samba time! @britishmuseum @camdentalking #Dis...               happy\n",
            "73   Lots of ancient artefacts to enjoy @britishmus...               happy\n",
            "76   Plus excellent prizes from the @britishmuseum ...               happy\n",
            "77   Feliz cumpleaÃ±os,Rubens! Happy birthday,Ruben...               happy\n",
            "81   . @NationalGallery What message does oil spons...               angry\n",
            "85   I Love Museums http://t.co/LblvLtgxmR via @cam...               happy\n",
            "86   .@IanLaveryMP can you attend adjournment debat...               angry\n",
            "88   Excellent exhibition at @britishmuseum on Napo...               happy\n",
            "92   @britishmuseum we love the badge! Here she is ...               happy\n",
            "98   This de Hooch 'Courtyard of a House in Delft' ...               happy\n",
            "104  A @heritagelottery grant helped us acquire 'Wh...               happy\n",
            "106  @britishmuseum It's amazing! You're always so ...               happy\n",
            "109                           @FitzMuseum_UK Yum! ð               happy\n",
            "111  @britishmuseum Of course! Nothing more and not...               happy\n",
            "114            @britishmuseum well deserved. Congrats!               happy\n",
            "115  @britishmuseum loved the exhibition today! My ...               happy\n",
            "116  @MicroPasts @britishmuseum task 40821 of Forei...               happy\n",
            "120  I got 10/10 in 'Can you guess the artwork?' ht...               happy\n",
            "125  A young friend has drawn us a great pic of @ke...               happy\n",
            "129  @nationalgallery #AskTheGallery When will you ...               angry\n",
            "130  Patients of Sharp Ward are enjoying historic v...               happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ex8gWi3_lvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}